import torch
from torch import nn

from model.base_model import BaseModel
from model.utils import sequence_mask


class BaseHALO(BaseModel):
    def __init__(self, halo_model, max_len, hidden_dim):
        super().__init__(param_file_name="base_halo.pt")
        self.halo = halo_model
        self.max_len = max_len
        self.proj = nn.Linear(halo_model.transformer.n_embd, hidden_dim)  # √©p v·ªÅ c√πng k√≠ch th∆∞·ªõc GRU c≈©

    def forward(self, x):
        # HALO tr·∫£ (B, T-1, V) ‚Üí ƒë·ªám timestep ƒë·∫ßu ƒë·ªÉ kh·ªõp PredictNextLoss
        code_probs = self.halo(x)
        B, T, V = x.shape
        out = torch.zeros(B, T, V, device=x.device)
        out[:, 0, :] = x[:, 0, :]
        out[:, 1:, :] = code_probs
        return out

    def calculate_hidden(self, x, lens):
        with torch.no_grad():
            mask = sequence_mask(lens, self.max_len).unsqueeze(-1).to(x.device)
            hidden = self.halo.transformer(x)  # (B, T', E)
            # üîß Pad hidden n·∫øu HALO tr·∫£ T' < max_len
            if hidden.size(1) < self.max_len:
                pad_len = self.max_len - hidden.size(1)
                pad = torch.zeros(hidden.size(0), pad_len, hidden.size(2), device=x.device)
                hidden = torch.cat([hidden, pad], dim=1)
                
            hidden = self.proj(hidden) * mask  # √©p E‚Üíhidden_dim v√† √°p mask
            return hidden
